# HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction

If you find this repository can help your research work, please cite the following paper:


Linyi Yang, Tin Lok James Ng, Barry Smyth, Ruihai Dong. HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction. Proceedings of the The Web Conference (WWW-20).

    @inproceedings{yang-2020-html,
    title = "HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction",
    author = "Linyi Yang, Tin Lok James Ng, Barry Smyth, Ruihai Dong",
    booktitle = "Proceedings of the 2020 The Web Conference (formerly WWW)",
    month = apr,
    year = "2020",
    address = "Taipei",
    publisher = "International World Wide Web Conferences Steering Committee",
    }
    
## Dataset    
The token-level transformer relies on the pre-trained transformers, which can be downloed from: [here](https://huggingface.co/)

## Model
We pride our both code and dataset. The HTML consists with token-level transformer and sentence-level transformer that are contained in the Model path. Also, we provide our experimental code of both Multi-task settings and Single-task settings under the Model folder.
