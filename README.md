# HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction

If you find this repository can help your research work, please cite the following paper:


Linyi Yang, Tin Lok James Ng, Barry Smyth, Ruihai Dong. HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction. Proceedings of the The Web Conference 2020.

    @inproceedings{yang-2020-html,
    title = "HTML: Hierarchical Transformer-based Multi-task Learning for Volatility Prediction",
    author = "Linyi Yang, Tin Lok James Ng, Barry Smyth, Ruihai Dong",
    booktitle = "Proceedings of the The Web Conference (formerly WWW) 2020",
    month = apr,
    year = "2020",
    address = "Taipei",
    publisher = "International World Wide Web Conferences Steering Committee",
    }
    
## Dataset    
The token-level transformer relies on the pre-trained transformers, which can be downloed from [here](https://huggingface.co/).
<br>The raw dataset of the earnings call can be found from [ACL-19, Qin and Yang](https://github.com/GeminiLn/EarningsCall_Dataset).

## Model
We provide our code and data used for the paper mentioned above. The HTML model consists with token-level transformer and sentence-level transformer that are contained in the Model path. Also, we provide our experimental code of both Multi-task settings and Single-task settings under the Model folder.

## Contact
Any questions or queries feel free to email me at linyi.yang@insight-centre.org -- Thanks for reading.
